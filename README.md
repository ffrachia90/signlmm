# ğŸ¤Ÿ SignLMM POC

**Proof of Concept del primer modelo multimodal de Lengua de SeÃ±as**

Desarrollado para **ELdeS** - Plataforma de aprendizaje de lengua de seÃ±as.

---

## ğŸ“‹ DescripciÃ³n

Este proyecto es una POC (Proof of Concept) para crear un modelo que pueda:

1. **Reconocer** seÃ±as a partir de video
2. **Clasificar** quÃ© seÃ±a se estÃ¡ realizando
3. **Traducir** las seÃ±as detectadas a espaÃ±ol natural

## ğŸ—ï¸ Arquitectura

```
Video de seÃ±as
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MediaPipe      â”‚  Extrae landmarks de manos y pose
â”‚  (Landmarks)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LSTM           â”‚  Clasifica la secuencia de landmarks
â”‚  Bidireccional  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM            â”‚  Traduce glosas a espaÃ±ol natural
â”‚  (GPT/Gemini)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
   "Hola, Â¿cÃ³mo estÃ¡s?"
```

## ğŸ“ Estructura del proyecto

```
signlmm-poc/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ videos/          # Videos originales de seÃ±as
â”‚   â”œâ”€â”€ landmarks/       # Landmarks extraÃ­dos (JSON)
â”‚   â””â”€â”€ processed/       # Dataset listo para entrenar
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ extract_landmarks.py   # Extrae landmarks de videos
â”‚   â”œâ”€â”€ prepare_dataset.py     # Prepara dataset para entrenar
â”‚   â”œâ”€â”€ train_model.py         # Entrena el clasificador
â”‚   â””â”€â”€ translate.py           # Traduce glosas a espaÃ±ol
â”œâ”€â”€ models/              # Modelos entrenados
â”œâ”€â”€ demo/
â”‚   â””â”€â”€ app.py          # AplicaciÃ³n web demo
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml     # ConfiguraciÃ³n
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸš€ InstalaciÃ³n

### 1. Clonar y crear entorno

```bash
cd signlmm-poc
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

### 2. Instalar dependencias

```bash
pip install -r requirements.txt
```

### 3. Configurar API keys (opcional, para traducciÃ³n LLM)

```bash
cp .env.example .env
# Editar .env con tu API key de OpenAI o Gemini
```

## ğŸ“Š Pipeline completo

### Paso 1: Preparar videos

Organiza tus videos en `data/videos/` con el formato:
```
{SEÃ‘A}_{usuario}_{toma}.mp4

Ejemplos:
HOLA_user01_take01.mp4
GRACIAS_user01_take01.mp4
YO_user02_take01.mp4
```

### Paso 2: Extraer landmarks

```bash
python scripts/extract_landmarks.py \
    --input data/videos \
    --output data/landmarks
```

### Paso 3: Preparar dataset

```bash
python scripts/prepare_dataset.py \
    --input data/landmarks \
    --output data/processed
```

### Paso 4: Entrenar modelo

```bash
python scripts/train_model.py \
    --data data/processed \
    --output models \
    --epochs 100
```

### Paso 5: Ejecutar demo

```bash
python demo/app.py
```

Abrir en el navegador: http://localhost:7860

## ğŸ® Modo demo (sin modelo)

Para probar la interfaz sin entrenar el modelo:

```bash
python demo/app.py --demo-mode
```

## ğŸ“ˆ MÃ©tricas esperadas

| Dataset | Accuracy esperada |
|---------|-------------------|
| 500 videos (50 seÃ±as) | 70-80% |
| 2,500 videos (50 seÃ±as) | 85-90% |
| 10,000 videos (100 seÃ±as) | 90-95% |

## ğŸ”§ ConfiguraciÃ³n

Editar `config/config.yaml` para ajustar:

- ParÃ¡metros del modelo (hidden_size, num_layers, etc.)
- Longitud de secuencia normalizada
- Learning rate y epochs
- Proveedor de LLM para traducciÃ³n

## ğŸ“ Formato de datos

### Videos de entrada
- Formato: MP4, WebM, AVI
- ResoluciÃ³n: MÃ­nimo 720p
- FPS: 30
- DuraciÃ³n: 2-10 segundos
- Contenido: Una seÃ±a por video, manos y cara visibles

### Landmarks extraÃ­dos (JSON)
```json
{
  "metadata": {
    "source_video": "HOLA_user01_take01.mp4",
    "fps": 30,
    "total_frames": 90
  },
  "landmarks": [
    {
      "frame": 0,
      "left_hand": [[x, y, z], ...],
      "right_hand": [[x, y, z], ...],
      "pose": [[x, y, z], ...]
    }
  ]
}
```

## ğŸ¤ Contribuir

1. Fork del repositorio
2. Crear branch: `git checkout -b feature/nueva-funcionalidad`
3. Commit: `git commit -am 'Agregar nueva funcionalidad'`
4. Push: `git push origin feature/nueva-funcionalidad`
5. Pull Request

## ğŸ“„ Licencia

Propiedad de ELdeS. Uso interno.

## ğŸ‘¥ Equipo

- **ELdeS** - Plataforma de aprendizaje de lengua de seÃ±as
- **Fernando Frachia** - Desarrollo POC

---

**SignLMM POC** | 2025 | ğŸ‡¦ğŸ‡·ğŸ‡ªğŸ‡¸ğŸ‡ºğŸ‡¾


